{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we shall write a quick script using Tweepy to gather the followers of several Twitter users. First, we need to grab our Twitter API credidentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import dill\n",
    "\n",
    "keys = dill.load(open('.secrets/api-keys.pkd', 'rb'))\n",
    "\n",
    "auth = tweepy.OAuthHandler(keys['API'], keys['API secret'])\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 889\n",
      "Rate limit reached. Sleeping for: 890\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 893\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 892\n",
      "Rate limit reached. Sleeping for: 891\n",
      "Rate limit reached. Sleeping for: 891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c70954a6538b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myang_followers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollowers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"AndrewYang\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0myang_followers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         data, cursors = self.method(cursor=self.next_cursor,\n\u001b[1;32m     74\u001b[0m                                     \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                                     **self.kargs)\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_cursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "yang_followers = []\n",
    "for page in tweepy.Cursor(api.followers, screen_name=\"AndrewYang\").pages():\n",
    "    yang_followers.extend(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yang_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yang_followers = dill.load(open('../scrapped_data/twitter-data/yang_followers.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "screen_names = []\n",
    "locations = []\n",
    "descriptions = []\n",
    "follower_counts = []\n",
    "friends_counts = []\n",
    "\n",
    "for follower in yang_followers:\n",
    "    names.append(follower.name)\n",
    "    screen_names.append(follower.screen_name)\n",
    "    locations.append(follower.location)\n",
    "    descriptions.append(follower.description)\n",
    "    follower_counts.append(follower.followers_count)\n",
    "    friends_counts.append(follower.friends_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'name': names, \n",
    "                   'screen_name': screen_names, \n",
    "                   'location': locations, \n",
    "                   'description': descriptions, \n",
    "                   'followers_count': follower_counts, \n",
    "                   'friends_count': friends_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1a5d6cd6ea08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mstate_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf_state_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7354\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7356\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7358\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   7391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7393\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m   7394\u001b[0m                              ' an index')\n\u001b[1;32m   7395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "states_full = [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "               \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n",
    "            \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "            \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "            \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "            \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "            \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "            \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n",
    "\n",
    "state_counts = defaultdict(int)\n",
    "\n",
    "for location in df.location.values:\n",
    "    for state in states:\n",
    "        if state in location:\n",
    "            state_counts[state] += 1\n",
    "    for i, state in enumerate(states_full):\n",
    "        if state in location:\n",
    "            state_counts[states[i]] += 1\n",
    "            \n",
    "df_state_counts = pd.DataFrame(state_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_count = sorted([(k, v) for k, v in state_counts.items()], key=lambda x: states.index(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.read_csv('../misc_data/state_populations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12125884])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pop[df_pop['State'] == 'California'].Percent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('AL', 109) is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-61c4eea7dac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfollowers_by_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpopulation_by_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'State'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstates_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPercent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m data = [ dict(\n",
      "\u001b[0;32m<ipython-input-73-61c4eea7dac4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfollowers_by_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpopulation_by_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'State'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstates_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPercent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m data = [ dict(\n",
      "\u001b[0;31mValueError\u001b[0m: ('AL', 109) is not in list"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "state_counts\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "followers_by_state = np.array([location[0] for location in ordered_count])\n",
    "population_by_state = np.array([df_pop[df_pop['State'] == states_full[states.index(location)]].Percent.values for location in ordered_count])\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = [location[0] for location in ordered_count],\n",
    "        z = [location[1] for location in ordered_count],\n",
    "#         locations = df['code'],\n",
    "#         z = df['total exports'].astype(float),\n",
    "        locationmode = 'USA-states',\n",
    "        text = [location[0] for location in ordered_count],\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            )\n",
    "        ),\n",
    "        colorbar = dict(\n",
    "            title = \"Millions USD\"\n",
    "        )\n",
    "    ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Number of Followers by State',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "init_notebook_mode(connected=True)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3192"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = []\n",
    "tweet_dates = []\n",
    "for tweet in tweets:\n",
    "    tweet_text.append(tweet._json['full_text'])\n",
    "    tweet_dates.append(tweet._json['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "http_tail = re.compile(\"https:.*$\")\n",
    "\n",
    "tweet = tweet_text[1]\n",
    "tweet_text = [re.sub(http_tail, '', text) for text in tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So, it has now been determined, by 18 people that truly hate President Trump, that there was No Collusion with Russia. In fact, it was an illegal investigation that should never have been allowed to start. I fought back hard against this Phony &amp; Treasonous Hoax!'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Text': tweet_text, 'Date': tweet_dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THANK YOU Texas, I love you!</td>\n",
       "      <td>Wed Apr 10 21:41:06 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join me in Crosby, Texas as I deliver remarks ...</td>\n",
       "      <td>Wed Apr 10 21:37:40 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, it has now been determined, by 18 people t...</td>\n",
       "      <td>Wed Apr 10 19:45:55 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spoke to Bibi @Netanyahu to congratulate him o...</td>\n",
       "      <td>Wed Apr 10 19:17:19 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump flags being waived at the Bibi @Netanyah...</td>\n",
       "      <td>Wed Apr 10 14:48:02 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @JudicialWatch: Judicial Watch President @T...</td>\n",
       "      <td>Wed Apr 10 03:02:43 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JudicialWatch: BREAKING: JW announced toda...</td>\n",
       "      <td>Wed Apr 10 02:59:40 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Everybody is now acknowledging that, right fro...</td>\n",
       "      <td>Wed Apr 10 02:22:07 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @IWV: .@POTUS @realDonaldTrump has \"deliver...</td>\n",
       "      <td>Wed Apr 10 02:05:47 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @HeyTammyBruce: Trump Job Approval Jumps to...</td>\n",
       "      <td>Wed Apr 10 02:00:46 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Democrats must end the loopholes on immigr...</td>\n",
       "      <td>Wed Apr 10 01:22:19 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>“The underlying issue remains the same without...</td>\n",
       "      <td>Wed Apr 10 01:01:51 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>THANK YOU, WORKING HARD!</td>\n",
       "      <td>Tue Apr 09 19:15:26 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Check this out - TRUTH!</td>\n",
       "      <td>Tue Apr 09 18:32:44 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Today, it was my great honor to welcome Presid...</td>\n",
       "      <td>Tue Apr 09 17:58:35 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>“She (Congresswoman Omar) keeps on assaulting ...</td>\n",
       "      <td>Tue Apr 09 16:09:58 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>Tue Apr 09 15:38:27 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“What’s completely unacceptable is for Congess...</td>\n",
       "      <td>Tue Apr 09 15:36:17 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>On National Former Prisoner of War Recognition...</td>\n",
       "      <td>Tue Apr 09 15:31:21 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Mainstream Media has never been more inacc...</td>\n",
       "      <td>Tue Apr 09 12:44:17 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>....I got along very well with Jerry during th...</td>\n",
       "      <td>Tue Apr 09 12:33:09 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Congressman Jerry Nadler fought me for years o...</td>\n",
       "      <td>Tue Apr 09 12:16:54 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The World Trade Organization finds that the Eu...</td>\n",
       "      <td>Tue Apr 09 11:34:39 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RT @GOPChairwoman: Our economy is on fire.\\n \\...</td>\n",
       "      <td>Tue Apr 09 04:51:09 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RT @WhiteHouse: The newly constructed 30-foot ...</td>\n",
       "      <td>Tue Apr 09 04:50:22 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Congratulations to Virginia - Great game!</td>\n",
       "      <td>Tue Apr 09 04:43:57 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RT @TeamTrump: BIGGER paychecks under Presiden...</td>\n",
       "      <td>Tue Apr 09 03:13:29 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A 9th Circuit Judge just ruled that Mexico is ...</td>\n",
       "      <td>Tue Apr 09 03:10:33 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Congratulations to the Baylor Lady Bears on th...</td>\n",
       "      <td>Mon Apr 08 23:57:29 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Democrats will never be satisfied, no matt...</td>\n",
       "      <td>Mon Apr 08 16:48:48 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>The Democrats are making a strong push to abol...</td>\n",
       "      <td>Sat Jun 30 11:07:30 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>RT @DanScavino: “@GovMikeHuckabee: Trump could...</td>\n",
       "      <td>Sat Jun 30 10:59:46 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>RT @realDonaldTrump: Six months after our TAX ...</td>\n",
       "      <td>Sat Jun 30 10:59:04 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>RT @realDonaldTrump: Before going any further ...</td>\n",
       "      <td>Sat Jun 30 10:57:05 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>RT @IvankaTrump: Thank you @SecPompeo. \\nIt wa...</td>\n",
       "      <td>Sat Jun 30 10:49:42 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>RT @IvankaTrump: Thank you Senator Alexander f...</td>\n",
       "      <td>Sat Jun 30 10:49:20 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>The new plant being built by Foxconn in Wiscon...</td>\n",
       "      <td>Fri Jun 29 23:30:02 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>RT @FoxBusiness: .@IvankaTrump: \"I think one o...</td>\n",
       "      <td>Fri Jun 29 23:21:33 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>RT @Scavino45: “@ICEgov New York operation lea...</td>\n",
       "      <td>Fri Jun 29 23:20:18 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>Six months after our TAX CUTS, more than 6 MIL...</td>\n",
       "      <td>Fri Jun 29 18:06:13 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>Before going any further today, I want to addr...</td>\n",
       "      <td>Fri Jun 29 17:59:22 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>Prior to departing Wisconsin, I was briefed on...</td>\n",
       "      <td>Thu Jun 28 20:49:55 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>AMERICA IS OPEN FOR BUSINESS! \\n</td>\n",
       "      <td>Thu Jun 28 19:09:42 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>Today, we broke ground on a plant that will pr...</td>\n",
       "      <td>Thu Jun 28 18:57:22 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>I am in Milwaukee, Wisconsin, for meetings. So...</td>\n",
       "      <td>Thu Jun 28 13:06:01 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>....persecuted on old and/or totally unrelated...</td>\n",
       "      <td>Thu Jun 28 12:56:17 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>When is Bob Mueller going to list his Conflict...</td>\n",
       "      <td>Thu Jun 28 12:43:25 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>Peter Strzok worked as the leader of the Rigge...</td>\n",
       "      <td>Thu Jun 28 12:30:31 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>Just watched @SharkGregNorman on @foxandfriend...</td>\n",
       "      <td>Thu Jun 28 11:38:40 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>Amy Kremer, Women for Trump, was so great on @...</td>\n",
       "      <td>Thu Jun 28 11:32:09 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3182</th>\n",
       "      <td>Russia continues to say they had nothing to do...</td>\n",
       "      <td>Thu Jun 28 11:25:15 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183</th>\n",
       "      <td>Lover FBI Agent Peter Strzok was given poor ma...</td>\n",
       "      <td>Thu Jun 28 11:02:01 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>...home addresses – putting these selfless pub...</td>\n",
       "      <td>Thu Jun 28 03:24:54 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>In recent days we have heard shameless attacks...</td>\n",
       "      <td>Thu Jun 28 03:24:54 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>Thank you North Dakota. Together, we are MAKIN...</td>\n",
       "      <td>Thu Jun 28 03:03:04 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>Just landed in North Dakota with @SenJohnHoeve...</td>\n",
       "      <td>Wed Jun 27 23:14:58 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>Today, it was my great honor to welcome Presid...</td>\n",
       "      <td>Wed Jun 27 21:18:03 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>Heading to North Dakota to fully stand with an...</td>\n",
       "      <td>Wed Jun 27 20:28:58 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>Statement on Justice Anthony Kennedy. #SCOTUS</td>\n",
       "      <td>Wed Jun 27 19:13:34 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>Today, I was thrilled to join student leaders ...</td>\n",
       "      <td>Wed Jun 27 17:33:03 +0000 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0                         THANK YOU Texas, I love you!    \n",
       "1     Join me in Crosby, Texas as I deliver remarks ...   \n",
       "2     So, it has now been determined, by 18 people t...   \n",
       "3     Spoke to Bibi @Netanyahu to congratulate him o...   \n",
       "4     Trump flags being waived at the Bibi @Netanyah...   \n",
       "5     RT @JudicialWatch: Judicial Watch President @T...   \n",
       "6     RT @JudicialWatch: BREAKING: JW announced toda...   \n",
       "7     Everybody is now acknowledging that, right fro...   \n",
       "8     RT @IWV: .@POTUS @realDonaldTrump has \"deliver...   \n",
       "9     RT @HeyTammyBruce: Trump Job Approval Jumps to...   \n",
       "10    The Democrats must end the loopholes on immigr...   \n",
       "11    “The underlying issue remains the same without...   \n",
       "12                            THANK YOU, WORKING HARD!    \n",
       "13                             Check this out - TRUTH!    \n",
       "14    Today, it was my great honor to welcome Presid...   \n",
       "15    “She (Congresswoman Omar) keeps on assaulting ...   \n",
       "16                                                        \n",
       "17    “What’s completely unacceptable is for Congess...   \n",
       "18    On National Former Prisoner of War Recognition...   \n",
       "19    The Mainstream Media has never been more inacc...   \n",
       "20    ....I got along very well with Jerry during th...   \n",
       "21    Congressman Jerry Nadler fought me for years o...   \n",
       "22    The World Trade Organization finds that the Eu...   \n",
       "23    RT @GOPChairwoman: Our economy is on fire.\\n \\...   \n",
       "24    RT @WhiteHouse: The newly constructed 30-foot ...   \n",
       "25            Congratulations to Virginia - Great game!   \n",
       "26    RT @TeamTrump: BIGGER paychecks under Presiden...   \n",
       "27    A 9th Circuit Judge just ruled that Mexico is ...   \n",
       "28    Congratulations to the Baylor Lady Bears on th...   \n",
       "29    The Democrats will never be satisfied, no matt...   \n",
       "...                                                 ...   \n",
       "3162  The Democrats are making a strong push to abol...   \n",
       "3163  RT @DanScavino: “@GovMikeHuckabee: Trump could...   \n",
       "3164  RT @realDonaldTrump: Six months after our TAX ...   \n",
       "3165  RT @realDonaldTrump: Before going any further ...   \n",
       "3166  RT @IvankaTrump: Thank you @SecPompeo. \\nIt wa...   \n",
       "3167  RT @IvankaTrump: Thank you Senator Alexander f...   \n",
       "3168  The new plant being built by Foxconn in Wiscon...   \n",
       "3169  RT @FoxBusiness: .@IvankaTrump: \"I think one o...   \n",
       "3170  RT @Scavino45: “@ICEgov New York operation lea...   \n",
       "3171  Six months after our TAX CUTS, more than 6 MIL...   \n",
       "3172  Before going any further today, I want to addr...   \n",
       "3173  Prior to departing Wisconsin, I was briefed on...   \n",
       "3174                   AMERICA IS OPEN FOR BUSINESS! \\n   \n",
       "3175  Today, we broke ground on a plant that will pr...   \n",
       "3176  I am in Milwaukee, Wisconsin, for meetings. So...   \n",
       "3177  ....persecuted on old and/or totally unrelated...   \n",
       "3178  When is Bob Mueller going to list his Conflict...   \n",
       "3179  Peter Strzok worked as the leader of the Rigge...   \n",
       "3180  Just watched @SharkGregNorman on @foxandfriend...   \n",
       "3181  Amy Kremer, Women for Trump, was so great on @...   \n",
       "3182  Russia continues to say they had nothing to do...   \n",
       "3183  Lover FBI Agent Peter Strzok was given poor ma...   \n",
       "3184  ...home addresses – putting these selfless pub...   \n",
       "3185  In recent days we have heard shameless attacks...   \n",
       "3186  Thank you North Dakota. Together, we are MAKIN...   \n",
       "3187  Just landed in North Dakota with @SenJohnHoeve...   \n",
       "3188  Today, it was my great honor to welcome Presid...   \n",
       "3189  Heading to North Dakota to fully stand with an...   \n",
       "3190     Statement on Justice Anthony Kennedy. #SCOTUS    \n",
       "3191  Today, I was thrilled to join student leaders ...   \n",
       "\n",
       "                                Date  \n",
       "0     Wed Apr 10 21:41:06 +0000 2019  \n",
       "1     Wed Apr 10 21:37:40 +0000 2019  \n",
       "2     Wed Apr 10 19:45:55 +0000 2019  \n",
       "3     Wed Apr 10 19:17:19 +0000 2019  \n",
       "4     Wed Apr 10 14:48:02 +0000 2019  \n",
       "5     Wed Apr 10 03:02:43 +0000 2019  \n",
       "6     Wed Apr 10 02:59:40 +0000 2019  \n",
       "7     Wed Apr 10 02:22:07 +0000 2019  \n",
       "8     Wed Apr 10 02:05:47 +0000 2019  \n",
       "9     Wed Apr 10 02:00:46 +0000 2019  \n",
       "10    Wed Apr 10 01:22:19 +0000 2019  \n",
       "11    Wed Apr 10 01:01:51 +0000 2019  \n",
       "12    Tue Apr 09 19:15:26 +0000 2019  \n",
       "13    Tue Apr 09 18:32:44 +0000 2019  \n",
       "14    Tue Apr 09 17:58:35 +0000 2019  \n",
       "15    Tue Apr 09 16:09:58 +0000 2019  \n",
       "16    Tue Apr 09 15:38:27 +0000 2019  \n",
       "17    Tue Apr 09 15:36:17 +0000 2019  \n",
       "18    Tue Apr 09 15:31:21 +0000 2019  \n",
       "19    Tue Apr 09 12:44:17 +0000 2019  \n",
       "20    Tue Apr 09 12:33:09 +0000 2019  \n",
       "21    Tue Apr 09 12:16:54 +0000 2019  \n",
       "22    Tue Apr 09 11:34:39 +0000 2019  \n",
       "23    Tue Apr 09 04:51:09 +0000 2019  \n",
       "24    Tue Apr 09 04:50:22 +0000 2019  \n",
       "25    Tue Apr 09 04:43:57 +0000 2019  \n",
       "26    Tue Apr 09 03:13:29 +0000 2019  \n",
       "27    Tue Apr 09 03:10:33 +0000 2019  \n",
       "28    Mon Apr 08 23:57:29 +0000 2019  \n",
       "29    Mon Apr 08 16:48:48 +0000 2019  \n",
       "...                              ...  \n",
       "3162  Sat Jun 30 11:07:30 +0000 2018  \n",
       "3163  Sat Jun 30 10:59:46 +0000 2018  \n",
       "3164  Sat Jun 30 10:59:04 +0000 2018  \n",
       "3165  Sat Jun 30 10:57:05 +0000 2018  \n",
       "3166  Sat Jun 30 10:49:42 +0000 2018  \n",
       "3167  Sat Jun 30 10:49:20 +0000 2018  \n",
       "3168  Fri Jun 29 23:30:02 +0000 2018  \n",
       "3169  Fri Jun 29 23:21:33 +0000 2018  \n",
       "3170  Fri Jun 29 23:20:18 +0000 2018  \n",
       "3171  Fri Jun 29 18:06:13 +0000 2018  \n",
       "3172  Fri Jun 29 17:59:22 +0000 2018  \n",
       "3173  Thu Jun 28 20:49:55 +0000 2018  \n",
       "3174  Thu Jun 28 19:09:42 +0000 2018  \n",
       "3175  Thu Jun 28 18:57:22 +0000 2018  \n",
       "3176  Thu Jun 28 13:06:01 +0000 2018  \n",
       "3177  Thu Jun 28 12:56:17 +0000 2018  \n",
       "3178  Thu Jun 28 12:43:25 +0000 2018  \n",
       "3179  Thu Jun 28 12:30:31 +0000 2018  \n",
       "3180  Thu Jun 28 11:38:40 +0000 2018  \n",
       "3181  Thu Jun 28 11:32:09 +0000 2018  \n",
       "3182  Thu Jun 28 11:25:15 +0000 2018  \n",
       "3183  Thu Jun 28 11:02:01 +0000 2018  \n",
       "3184  Thu Jun 28 03:24:54 +0000 2018  \n",
       "3185  Thu Jun 28 03:24:54 +0000 2018  \n",
       "3186  Thu Jun 28 03:03:04 +0000 2018  \n",
       "3187  Wed Jun 27 23:14:58 +0000 2018  \n",
       "3188  Wed Jun 27 21:18:03 +0000 2018  \n",
       "3189  Wed Jun 27 20:28:58 +0000 2018  \n",
       "3190  Wed Jun 27 19:13:34 +0000 2018  \n",
       "3191  Wed Jun 27 17:33:03 +0000 2018  \n",
       "\n",
       "[3192 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation \n",
    "\n",
    "n_features = 500\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.99, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.fit_transform(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: united states year court looking supreme tonight end kavanaugh foxnews\n",
      "Topic #1: rt realdonaldtrump today president trump american great amp whitehouse people\n",
      "Topic #2: fbi meeting hillary new crooked clinton york times comey amp\n",
      "Topic #3: great thank state vote house republican congratulations new florida governor\n",
      "Topic #4: news fake amp media collusion witch hunt russia trump mueller\n",
      "Topic #5: border wall democrats security country amp want people stop southern\n",
      "Topic #6: trade china countries deal dollars tariffs world farmers billions nato\n",
      "Topic #7: president just people law trump did like don time 000\n",
      "Topic #8: north soon good korea jobs better forward look level way\n",
      "Topic #9: great america strong crime make total endorsement military job vets\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "dill.dump(tweets, open('tweets-1.pkb', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for page in range(1, 16 + 1):\n",
    "    tweets = tweets + api.user_timeline('HillaryClinton', count=200, tweet_mode='extended', page=page)\n",
    "    time.sleep(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(tweets, open('clinton-tweets.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = []\n",
    "tweet_dates = []\n",
    "for tweet in tweets:\n",
    "    tweet_text.append(tweet._json['full_text'])\n",
    "    tweet_dates.append(tweet._json['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_tail = re.compile(\"https:.*$\")\n",
    "\n",
    "tweet = tweet_text[1]\n",
    "tweet_text = [re.sub(http_tail, '', text) for text in tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Text': tweet_text, 'Date': tweet_dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ve family night got best 000 stand military let thanks\n",
      "Topic #1: people young future year million hfa man woman new old\n",
      "Topic #2: watch america world live tax matter white great tune class\n",
      "Topic #3: trump donald rt hillary president debate just timkaine campaign like\n",
      "Topic #4: make vote women rights sure america today college health plan\n",
      "Topic #5: families children kids states working need help administration united chip\n",
      "Topic #6: day join thank candidates office today new election gun friend\n",
      "Topic #7: trump say change climate pence democracy clear president won need\n",
      "Topic #8: amp years hillary going country days care just let work\n",
      "Topic #9: rt hillary clinton hillaryclinton president flotus vote good potus candidate\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.99, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(tweet_text)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(tweet_text)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
